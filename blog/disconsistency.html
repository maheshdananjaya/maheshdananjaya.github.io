<!DOCTYPE html>
<html>
	<head>
		<title>Something is not quite right about distributed consistency? Chaos everywhere!</title>
	</head>
	<body>
		<p> <strong> Something is not quite right about distributed consistency? Chaos everywhere!</strong>
			<br/>
			<br/>
<strong>Motivation</strong>- Let’s dive into the problem directly. I have heard people using linearizability when discussing distributed systems’ consistency. But linearizability is a bit indistinct (undefined) consistency model for these systems because it has never been formalized in that domain. I have also heard people using linearizability to explain consistency in CAP and consensus protocols which is IMO wrong. Not everyone uses linearizability: instead (a few) seasoned researchers use strong consistency which is sensible but has made many people confused. In pursuit of their effort to interpret, or define, the term strong consistency, they have all fallen into the same trap of a very cheeky mistake. These are some of the well-established confusions (or perhaps myths) :) that are established due to this ambiguity of 'strong consistency'. I will try to clear out some of those. I am also looking to find answers/feedback to these questions.  
			<br/>
			<br/>
Confusion 1: ‘C’ in CAP is linearizability. (also Brewer called it linearizable :) ).
			<br/>
Confusion 2: Strong consistency in consensus (Paxos) protocols is linearizability. (p.s. I know, raft too has used lineaizability)
			<br/>
Confusion 3: Strong consistency of a replicated single object always means linearizability.
			<br/>
Confusion 4: 2PC is only for transactions that have multiple different participants (like different objects) resiging on distinct machines. 
			<br/>
Confusion 5: 2PC is always blocking thus we need 3PC.
			<br/>
			<br/>
<strong>Linearizability Definition</strong>- Each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response hence enforcing real-time ordering of the operations. Example?. 	
			<br/>
			<br/>
<strong>Linearizability</strong>- First of all, linearizability is defined in order to govern 'consistency' of a single object when multiple (concurrent) actors are acting on it; it is a client-centric consistency model. As a subsequent outcome, Linearizability has been successfully used in the shared memory world for years, particularly in cache coherence protocols. For instance, invariants like single-writer-multiple-reader(SWMR) are the basic building blocks of almost all the multi-core processors today. Let’s not go into this for now. However linearizability is not accurately defined in the domain of distributed systems, specifically with respect to failures (except for some recent work in persistence, durable linearizability if you are interested).  When it comes to distributed systems failures are inevitable. These systems are often replicated with multiple copies of the same object/data; their prime design goal is provide availability and fault-tolerance. 
			<br/>
			<br/>
<strong>Problem?</strong>. What's the problem of linearizability in distributed systems? Distributed systems rely on replication to make the service available in the presence of faults. However, linearizability is not defined in the world of failures and/or multiple-copies; Linearizability is defined in an ideal world with no failures when there is a single copy of data. For instance, what happens to any operation (read/write/read-modify-write) if something goes wrong, or fails, in between its invocation and completion. How can the operation be completed if something fails in the middle. In the spirit of distributed (replicated) systems, Linearizability has the following three main problems here. First (1) <font color="#0000ff">(failure) atomicity</font> of multiple copies, or reasoning about multiple copies of the same data, second (2) <font color="#0000ff">recovery</font>, how to rollback, or rollforward, partial data of a failed operations, third, (3) <font color="#0000ff">client acknowledgements</font>, will the client ever receive a response/acknowledgement in the foreseeable future if something fails (even after the operations is successful).
			<br/>
			<br/>
<strong>Synchronous Replication (No partitions)</strong>: For now, let's say you have a system with bounded delays and no network paritions. That means when something fails you know it. Let's say you have a database of a single object, X , and it is replicated; X has multiple copies (Let's say 3, X’, X’’, X’’’) stored in three distinct servers. This is in the spirit of a modern replicated database. Clients can read read/write from any of the replicas. Assume for instance that the client reads from just one copy (i.e local replica if the clients are running on any replica server). 
			<br/>
			<br/>
<strong>Strong Consistency?</strong> Let's assume we need strong consistency here, i.e real-time operations; the effects of the operation will be visible to all readers (atomicaly) in real time if at least one of them reads the new value. For instance, if client C1 writes the X=1 then if client C2 reads X=1, all other clients must read X=1; this means all copies of X– X’,X’’ and X’’’--’must be updated atomically. How can we do this?. To do this all replicas should reach an agreement over the order of updates to the objects.  (I use agreement to make it more general. not to be confused with agreement in consensus. I will come to this later)
			<br/>
			<br/>
We can enforce this agreement with simple atomic-commitment, basically transactions, because now a write operation is a transaction that atomically updates all copies of X: X’, X’’, X’’’. A Write X can be interpreted as {WR(X’), WR(X’’), WR(X’’’)}. Read X is  reading from any copy, for example  Read (X’’). However the term 'aotmic-commitment' is confusing here. Becuase 'atomic-acommit' is often related with transaction tht has mutiple reads/writes. Therefore, atomic commitment here is not to be confused with transactions with multiple operations/participants that opertes on different paritions. To avoid this confision, researchers have used <font color="#0000ff">atomic consistecy</font> to describe single objects replicated systems. Its good understand these two tyeps of atomicity: single replicated objects (<font color="#0000ff">type-1</font>) vs mutiple replicated/non-replicated objects (<font color="#0000ff">type-2</font>). Transactions with ACID and the term 'atomic-commiment' have been often used to describe the latter (i.e. type-2). But people have forgotten that the former also has simlar properties writes/RMWs. Additionally, read-modify-write(RMW) on a single object indeed has two operations-- read and write to the same object-- which slighly overlaps with and very close to type-2, but still the same object (to me, there is no diference as long as the operations are to the same replicated parition). From here onwards, I use <u> agreement, atomic-conisistency, or atomic-commitment (type-1)</u>, to describe strong cosistency with read/write/rmw on a single object. (Why this link has been brocken? I don't know :)) .
			<br/>
			<br/>
 (todo- add 2 figures with two types of atomcity. for read/writes and then rmws. former is becuase mutiple copies. latter is becuase mutiple operations. they can both be handles the same way becuase of real-time ordering).
 
			<br/>
			<br/>
In fact a simple (popular) <font color="#0000ff">atomic-commitment</font> protocol, optimistic <font color="#0000ff">two-phase commit (2PC)</font>, can be used to guarantee this strong consistency. Additionally, this 2PC protocol for single object/partition replication is non-blocking; we do not need 3PC (there is another case when 2PC is non-blocking, i.e. single/multi-partition transactions with a reliable storage layer. I will come to this later). 
			<br/>
			<br/>
Database community has been using this approach, 2PC, for years to replcate single-server databases. I would recommend reading some of Phil Bernstein’s early <a href="">work</a>. But people have slightly misunderstood this approach with multi-partition transactions. This original 2PC protocol enforces <font color="#ff0000">single-copy, or one-copy, serializability</font> that is equivalent to so-called <font color="#ff0000">(strict) serializability or ACID</font> in distributed systems. (you can think of it as transactions with either a single read or write operation. People often get confused with transactions that have multiple operations/objects). Not only reads/writes but also RMWs to the same object, or even mini transactions within the same partition, can be achieved this way. In fact, in the synchronous world, there is <font color="#ff0000">no difference</font> in handling writes and RMWs. Keep this in mind. Need this in consensus. Lets only focus on read/write/RMW here).
			<br/>
			<br/>
<strong><u>Two observations</u></strong>: In the synchronous world, <em>agreement, or atomic consistency, could be achieved with a simple atomic-commitment</em>, or transactional, protocol like 2PC and <em> there is no difference between a simple write operation and any read-modify-write(RMWs).</em>
			<br/>
			<br/>
<strong>Is this “strong consistency” Linearizability?</strong>-  It is not linearizability though it has the real-time ordering property. It looks very close to linearizability specially without failures but it is not because readers can read from multiple replicas. This simple optimistic 2PC protocol actually gives more than linearizability. It provides atomicity, recovery and client-acknowledgements in the face of failures which linearizability does not offer. So it is ok to call this single-copy serializability as strong consistency, not linearizability.
			<br/>
			<br/>
<strong>CAP theorem</strong> -  CAP -- which stands for Consistency, Availability and (Network) Partitions -- says you cannot have all three properties at once. You can think of our simple optimistic 2PC protocol as a way to substantiate CAP theorem. Because you can simply show that if there are network partitions, this simple 2PC cannot forward progress. For instance, If you assume there is a network partition, writes cannot atomically update all replicas, hence only affording C or A: either client reads are strongly consistent but not available and vice versa. For instance, clients can read from its stale local copy (i.e. AP) or  client’s write operation is blocked until the partition goes away (i.e CP) and it connects with all the replicas. You can work  it out with the 2PC. It's a good exercise. CAP theorem seems to be a result of some early replicated database <a>work</a>. The consistency it assumes appears to be strong serilizability or single-copy serilizability. (which is IMO compliant with ACID if you define A right though the link has been unclear. Becuase A in ACID is defined for transactions:)). 
			<br/>
			<br/>
<strong>C in CAP is not Linearizability</strong> - As you can see, C in  CAP theorem is not linearizability. It is supposed to be single-copy serializability. But the CAP <a href="https://www.cs.princeton.edu/courses/archive/fall19/cos418/papers/cap.pdf">paper</a>, which was not written by Brewer btw, has  <font color="#ff0000">tentatively</font> used ‘linearizability’ alongside single-copy serializability!. Since then some researchers have started calling that C is Linearizability. Which is IMO inaccurate. Here what the paper says: "The most natural way of formalizing the idea of consistency service is as an atomic object. <font color="#006400">Atomic</font> [citation],  <font color="#ff0000">or linearizability</font> [citation], is the condition expected by most web sercies today **(note to what atomic consistency is and how its different from A in ACID becuase A is defined for transactions)" :). However, note that the later <a href="https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf">versions</a> doesn't seems to have linearizability.
			<br/>
			<br/>
<strong>Asynchrony with network partitions</strong> - When there are network partitions, client write operations can’t reach all the replicas. Additionally, the client cannot continue with the remaining replicas because the client may be reading from isolated replicas. For example, if a write operation is completed with only 2 replicas in the above example, the remaining replica can still serve the old values of X; X’’’ can be stale which is not strong consistency (real-time ordering is not there because one client can see the new value of X from X’ and another one can see the old value from X’’’ for example). This is the basis of a quorum which I will explain after the next one. 
			<br/>
			<br/>
<strong>2PC is “Anti-Availability”</strong> - One way to overcome the above mentioned problem is to simply remove the unresponsive replica from the replica group. But it is not as easy as it sounds. For instance, network partitions can make it worse; you need reliable failure detectors here. Even if you can do this, other replicas or clients need to agree on the new configuration without the lost replica. This may require stop-the-world kind of reconfigurations to stop all clients to change the replica configuration. This is why 2PC is called the anti-availability protocol. Check Pat Helland’s note here []. Who is this guy?  I love this guy :). I wish I have seen this earlier. There are several ways to overcome this problem. Quorums are the most popular. Check papers like Spanner. (There are other ways like primary-backup or chain replication. IMO, primary-backup replication is undereeated in today's world.)
			<br/>
			<br/>
<strong>Quorums</strong> - One way to avoid 2PC anti-availability problem is to continue the write operations with the remaining replicas--for example if one fails, continue with only 2 in our case; write operations rather continue without the lost replica. This is the foundation of quorums. The idea is: no need to worry about failures, just continue with whatever you have. But everyone needs to agree on what is remaining.  This way you can tolerate partitions and avoid global reconfigurations. But of course, you cannot read from one, or your local, replica. You can see this as CP in CAP because, in some sense, the reads can't be read from any single replica available; instead you need to read at least one replica that has the new value. (Note: There are some issues with CAP. This is one of them. But there are extended models which I am not going to discuss here).
			<br/>
			<br/>
<strong>Majority</strong> - If the write operation continues with some, or a partial set of replicas, the client's read must somehow overlap with at least one of them. This is why you need the majority for your write operations. If the client waits for only 2 replicas, assuming one fails, read needs to read from 2 replicas. If we generalize this, you can think of it as W +R > N. 
			<br/>
			<br/>
Interpretations- For instance, if you have no failures, the write operation can reach all N replicas (W=N), then the client reads from its local (R=1), then N+1 > N. If there is only one failure that could happen, W must be N-1 and in that case, R must be at least 2. Usually we do not know how many failures can happen. So we can proceed with the majority. For instance, the write reaches at least N/2+1 replicas, Read should read from N/2+1 (For any integer N>3, R=N/2+1 and W=N/2+1 implies R+W>N).
			<br/>
			<br/>
//{TODO- you can prove that systems with failures can never achieve R=1. Same equation can be used to prove that we need an atomic commit for when R=1.} 
			<br/>
			<br/>
<strong>Majority is not “Strong Consistency”</strong> - Simple majority based quorum protocol is not strongly consistent (For linearizable folks, it's not linearizable). It violates real-time property without additional actions. 
			<br/>
			<br/>
Let's take our example with 3 replicas. If we use the majority every write (and read as well) operation needs to wait for 2 replicas to acknowledge before completing the operation. Let's assume client C1 is waiting for X in X’ and X’’ in our example, but not X’’’ due to network partitions. While updating those replicas, X’ and X’’ can well be 0 and 1. If another client C2 reads X=0 from X’ and X=1 from X’’ which it interprets as X=1(the newest value), it will see the new value of X. But if there is another client C3 that reads from X=0 from X’ and X=0 from X’’’ due to unresponsive X’’ (where X=1), then C3 only sees the old value X=0. You see, at a given time, two clients can see two different values which violate real-time property. This could be seen as an atomicity violation as the majority of replicas are not converged due to network delays.      
			<br/>
			<br/>
(NOTE- without majority quorums are not strongly consistent at all. For example check N>=5, W=(N-1), R=2). 
			<br/>
			<br/>
<strong>Read-Repair for Strong Consistency</strong>: However, we do not need atomic commitment to solve the above problem. For instance, in this example, if C2 sees the new value from X’’=1 it can wait and update X’ from its side which is called read-repair. This could simply provide real-time, hence atomicity, order to the write operations. C2 only continues and uses X=1 once it knows the majority is X=1. This way even if C reads X=0 from X’, that operation appears to have happened before C2’s read. Simple. This is the basic idea of ABD and MW-ABD protocol. It provides read/Write registers over quorum of replicas. (There are some problems with ABD/MW-ABD that I still do not fully understand.  In addition, some people call it linearizable. To further investigate, I checked in Lynch’s slidest at MIT. Prof.Lynch has never used, or called,  that ABD offers  lineraizability :). It says well-formedness and atomicity)
			<br/>
			<br/>
<strong>Quorums alleviates ‘agreement’ for Reads/Writes </strong> - One observation in majority based quorums is that, because clients by-default read from more than one replica, it does not need agreement for simple reads/writes (recall in synchronous world both write and RMWs required agreement and we used atomic commitment or 2PC). For instance, Writes do not  need to block readers in order to atomically update all replicas (all copies of the same object). This is powerful; in other terms you do not need ‘agreement’ or ‘consensus’. This is an <font color="#ff0000">implementation artefact</font> of quorums because the read operation reads more than one replica. However, anything more than simple reads/writes, like read-modify-write (RMW), needs agreement between the order of operations (for instance the read and write in RMWs) in all copies.  It is proven in FLP impossibility.
			<br/>
			<br/>
Confusion? - Recall that atomic commitment is two-fold; committing multiple copies of the same object (type-1) or committing different objects(type-2). People have often used atomic-commitment for the latter.  But the former is also a form of atomic commitment which I have called ‘agreement or atomic-consistency’. However this method is only required for a single object/partition replicated database. 
			<br/>
			<br/>
<strong>Consensus = agreement (a form of atomic-commitment) over quorums</strong> - One last piece of the puzzle. We need ‘agreement’, or IMO, a form of atomic commitment, for any operation other than simple writes/reads to agree on the order of two (or multiple) operations in multiple copies of the object/data. Paxos proposes a complete consensus protocol that offers strong consistency; Paxos archives atomic-commitment over quorums. I think the way to realize this consistency model (as systems person) is as (strict) serializability because Paxos provides the real-time property and atomicity (multiple copies of the same object) for the operation. Not only that, it also provided recovery that does not stop the entire service. This is why Paxos is highly available and is the building block of almost all the distributed databases today. 
			<br/>
			<br/>
It might be confusing to use (strict) serializability because people often think strict serializability is only for transactions that have multiple objects/participants. That is why I believe people use ‘strong consistency’ (and linearizability in some cases) as the way to mimic, or perhaps intepret, its consistency; it actually implies real-time ordering and atomicity (multiple-copies of the same object). To me, Paxos looks like an (optimized) single-object transaction protocol. If you actually look at Paxos, it looks like an extension of the 2PC protocol I described before with an additional phase. 
			<br/>
			<br/>
(Similar to that 2PC protocol, Paxos, with some tweaks, should be able to support replicated single-partition mini-transactions with multiple objects. Note that RMW is two operations: but a read and a write to the same object. Anything more than a single partition needs multi-partition transactions. I am not gonna talk about it here. There is some work like 3PC, PAC etc. Also I like how systems like Spanner use consensus to make each partition highly available while running a multi-partition 2PC protocol for transactions. This is the other case apart from single-partition replication that 2PC is non-blocking. Check Spanner. it uses consensus within each replaictaed parition while using 2PC for transactions over different partitions. nice design!)  
			<br/>
			<br/>
<strong>Conclusion and Questions</strong>
			<br/>
There is a common misunderstanding, or confusion, about distributed systems consistency. Using linearizability as a way to describe available, or replicated, systems that serve client requests from different replicas, though compelling, it is not complete. The term strong consistency has come to the rescue, but many people, especially people in the industry, have (mis)interpreted it slightly differently.  Only a few people (like Brewer I guess :)) know about this. So next time you use strong consistency or linearizability, use it wisely. This is my opinion based on my research. Maybe there is something that I have missed or misunderstood. I welcome any comments, thoughts, or any constructive criticism. 
			<br/>			
			<br/>

Conclusion 1: ‘C’ in CAP is NOT linearizability. (also Brewer DID NOT called it linearizable :) ).
			<br/>
Conclusion 2: Strong consistency in consensus (Paxos) protocols is NOT linearizability. (though raft has used lin)
			<br/>
Conclusion 3: Strong consistency over a single object always DOES NOT mean linearizability. Its sometimes called, atomic consistency, single-copy serializability or strong consistency.
			<br/>
Conclusion 4: 2PC is NOT  only for transactions with multiple operations/participants. It can be ant aotmic commitment, e.g. single object replicated KVS. 
			<br/>
Conclusion 5: 2PC is NOT always blocking thus we need 3PC. 2PC is not blocking for single-parition single object replication, or with a reliable replication layer that is available (E.g Spanner, TiDB). 
			
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
		</p>
	</body>
</html>
