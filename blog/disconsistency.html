<!DOCTYPE html>
<html>
	<head>
		<title>Something is not quite right about distributed consistency? Chaos everywhere!</title>
	</head>
	<body>
		<p> <strong> Something is not quite right about distributed consistency? Chaos everywhere!</strong> 
			<br/>
			<br/>
<strong>Motivation</strong>- Let’s dive into the problem directly: the term "linearizability" is often used when discussing <a href="https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html">consistency</a> in distributed systems. However linearizability is an indistinct (undefined) consistency model for these systems because it has never been formalized in that domain. Some people also mistakenly use linearizability to explain consistency in CAP and consensus protocols. It's important to note that not everyone relies on linearizability; instead, some seasoned researchers prefer the concept of strong consistency, which is a more sensible choice. However, this has caused confusion for many people. In an attempt to clarify the term "strong consistency," we have all fallen into the same trap of making the same mistake. There are several established misunderstandings, or myths, that exist due to the ambiguity of this term. In this article, I aim to clear up some of these confusions and seek feedback on some unanswered questions..  
			<br/>
			<br/>
Confusion 1: ‘C’ in CAP is linearizability. (also Brewer called it linearizable :) ).
			<br/>
Confusion 2: Strong consistency in consensus (like Paxos) protocols is linearizability.
			<br/>
Confusion 3: Strong consistency of a replicated single object always means linearizability.
			<br/>
Confusion 4: 2PC is only for transactions that have multiple different participants (like different objects) resiging on distinct machines. 
			<br/>
Confusion 5: 2PC is blocking so we always need 3PC.
			<br/>
			<br/>
<strong>Linearizability (Definition</strong>- Each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response hence enforcing real-time ordering of the operations. Example?. 	
			<br/>
			<br/>
<strong>Linearizability</strong>- To begin, linearizability is a consistency model that governs the consistency of a single object when multiple actors concurrently act on it. It is a client-centric model that has been successfully used in the shared memory world for years, particularly in cache coherence protocols. Invariants like single-writer-multiple-reader (SWMR) are basic building blocks of almost all multi-core processors today. However, the definition of linearizability is not entirely accurate in the domain of distributed systems, especially when it comes to failures, except for some recent work in persistence, which introduced the concept of durable linearizability. In distributed systems, failures are inevitable, and the systems are often replicated with multiple copies of the same object/data to provide availability and fault-tolerance. Thus, the definition and use of linearizability in distributed systems are more complex and nuanced. 
			<br/>
			<br/>
<strong>Problem</strong>. The problem with linearizability in distributed systems lies in the fact that these systems rely on replication to maintain service availability in the presence of faults. However, linearizability is not defined for scenarios involving failures or multiple copies. Linearizability is defined for an ideal world with no failures, where only a single copy of data exists. The question arises, what happens to an operation (for instance read/write/read-modify-write) if something goes wrong between its invocation and completion? How can an operation be completed if something fails in the middle?
			<br/>
Specifically, in the context of distributed (replicated) systems, linearizability faces three main problems: first (1), <font color="#0000ff">(failure) atomicity</font> of multiple copies, or reasoning about multiple copies of the same data, second (2), <font color="#0000ff">recovery</font>, or how to rollback or roll forward partial data of a failed operation, and third (3), <font color="#0000ff">client responses (acknowledgements)</font>, or whether the client will receive a response in the foreseeable future, even if the operation is successful.
			<br/>
			<br/>
<strong>Synchronous Replication (No partitions)</strong>: Let's assume that we are dealing with a system that has bounded delays and no network partitions, which means that when something fails, we can detect it. Suppose we have a database consisting of a single object, X, that is replicated. In this scenario, X has multiple copies (let's say 3: X', X'', X''') stored on three distinct servers. This is in keeping with the design of modern replicated databases, where clients can read from or write to any of the replicas. For the purposes of this discussion, let's assume that the client reads from just one copy (i.e., the local replica if the client is running on any replica server). 
			<br/>
			<br/>
<strong>Strong Consistency?</strong> Assuming we require strong consistency for real-time operations, the effects of an operation should be visible to all readers atomically in real time, provided at least one of them reads the new value. For example, if client C1 writes X=1, then if client C2 reads X=1, all other clients must also read X=1. This means that all copies of X – X', X'', and X''' – must be updated atomically. But how can we achieve this? In order to do so, all replicas must reach an agreement on the order of updates to the objects. (I am using the term "agreement" here to make it more general and to avoid confusion with the concept of agreement in consensus, which I will explain later.)
			<br/>
			<br/>
We can enforce the agreement over the order of updates to the objects with simple atomic-commitment, which is essentially a transaction that atomically updates all copies of X: X’, X’’, X’’’. A Write X can be interpreted as {WR(X’), WR(X’’), WR(X’’’)}, and Read X is reading from any copy, for example, Read (X’’). However, the term 'atomic-commitment' can be confusing here because it is often related to transactions with multiple reads/writes. Therefore, atomic commitment here should not be confused with transactions with multiple operations/participants that operate on different partitions. To avoid this confusion, researchers have used the term <font color="#0000ff">atomic consistency</font> to describe single-object replicated systems. It's important to understand these two types of atomicity: single replicated objects (<font color="#0000ff">type-1</font>) vs multiple replicated/non-replicated objects (<font color="#0000ff">type-2</font>). Transactions with ACID and the term 'atomic-commitment' have often been used to describe the latter (i.e. type-2). But people have forgotten that the former also has similar properties like writes/RMWs. Additionally, read-modify-write(RMW) on a single object indeed has two operations -- read and write to the same object -- which slightly overlaps with and is very close to type-2, but still the same object (to me, there is no difference as long as the operations are on the same replicated partition). From here onwards, I use <u> agreement, atomic-consistency, or atomic-commitment (type-1)</u> to describe strong consistency with read/write/RMW on a single object. (Why this link has been broken? I don't know :)).
			<br/>
			<br/>
 (TODO - add 2 figures with two types of atomcity. for read/writes and then rmws. former is becuase mutiple copies. latter is becuase mutiple operations. they can both be handles the same way becuase of real-time ordering).
 
			<br/>
			<br/>
A simple <font color="#0000ff">atomic-commitment</font> protocol, specifially <font color="#0000ff">two-phase commit (2PC)</font>, can be used to ensure strong consistency in single object/partition replication. The beauty of using <a href"https://people.eecs.berkeley.edu/~culler/papers/dds.pdf">2PC<a> in this context is that it is non-blocking. Therefore, there is no need for a 3PC, which is required in other scenarios. It is worth noting that 2PC is also non-blocking in single/multi-partition transactions with a reliable storage layer, which I will discuss later.
		
			<br/>
			<br/>
The database community has been using the simple optimistic two-phase commit (2PC) protocol for years to replicate databases (see link1, link2, etc.). For those interested, I would recommend reading some of Phil Bernstein's early work on this topic. However, there has been some confusion around this approach when it comes to multi-partition transactions. The original 2PC protocol enforces <font color="#ff0000">one-copy, serializability</font>, which is equivalent to the so-called <font color="#ff0000">(strict) serializability or ACID</font> in distributed systems. It's important to note that this protocol can handle not only reads and writes but also RMWs to the same object, or even mini transactions within the same partition. In fact, in the synchronous world, there is <font color="#ff0000">no difference</font> in handling writes and RMWs. It's important to note that we only dicuss read/write/RMW operations here.
			<br/>
			<br/>
<strong><u>Two observations</u></strong>: In the synchronous world, <em>agreement, or atomic consistency, could be achieved with a simple atomic-commitment</em>, or transactional, protocol like 2PC and <em> there is no difference between a simple write operation and any read-modify-write(RMWs).</em> (some <a href="https://cse.buffalo.edu/~stevko/courses/cse486/spring14/lectures/28-replicated_transactions.pdf">slides<a>)
			<br/>
			<br/>
<strong>Is this “strong consistency” Linearizability?</strong> - While the optimistic 2PC protocol provides real-time ordering and may appear similar to linearizability, it is not equivalent because readers can read from multiple replicas. However, it does provide atomicity, recovery, and client acknowledgements in the face of failures, which are not offered by linearizability. In fact, this protocol provides more than linearizability, which is why it is sometimes referred to as one-copy serializability or strong consistency. So, while it may not be linearizable, it is still a strong consistency model that offers valuable properties.
			<br/>
			<br/>
<strong>CAP theorem</strong> -  The CAP theorem (Consistency, Availability, and Network Partitions) states that you cannot achieve all three properties simultaneously. Our optimistic 2PC protocol can be used to demonstrate this theorem. If there are network partitions, the protocol cannot make progress. For example, if there is a network partition, writes cannot atomically update all replicas, making it possible to achieve only consistency or availability. Clients can either read from their stale local copy (AP) or their write operation can be blocked until the partition goes away (CP) and they connect with all replicas. You can verify this by working through the 2PC. CAP theorem is believed to have originated from early work on replicated databases. The consistency it assumes is one-copy serializability, which is strong consistency compliant with ACID if you define A correctly, although the link between CAP and ACID has been unclear. This is because A in ACID is defined for transactions. 
			<br/>
			<br/>
<strong>C in CAP is not Linearizability</strong> - As you can see, C in the CAP theorem is not linearizability. It is supposed to be (one-copy) <a href='https://www.cl.cam.ac.uk/teaching/1213/Databases/db2013_L11_L12_2up.pdf'>serializability</a>. However, the CAP <a href='https://www.cs.princeton.edu/courses/archive/fall19/cos418/papers/cap.pdf'>paper</a>, which was not written by Brewer, has tentatively used ‘linearizability’. Since then, some researchers have inaccurately referred to C as linearizability. Here is what the paper says: 'The most natural way of formalizing the idea of a consistency service is as an atomic object. <font color='#006400'>Atomic</font> [citation], <font color='#ff0000'>or linearizability</font> [citation], is the condition expected by most web services today **(note what atomic consistency is and how it differs from A in ACID because A is defined for transactions).' However, note that later <a href='https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf'>versions</a> do not seem to include linearizability.
			<br/>
			<br/>
<strong>Asynchrony with network partitions</strong> - When a network partition occurs, client write operations are unable to reach all replicas, leaving the remaining replicas isolated and potentially serving stale data. This lack of strong consistency, where real-time ordering is not guaranteed for all clients, is the reason why a quorum is necessary. A quorum is a subset of replicas that is capable of processing write (or read) operations and guarantees that only the latest value of a data item is accepted. For more information, see <a href="https://www.cl.cam.ac.uk/teaching/1213/Databases/db2013_L11_L12_2up.pdf">this</a> and <a href="lsd.ls.fi.upm.es/Members/papers/2003/tods03.pdf">this</a>.

When there are network partitions, client write operations can’t reach all the replicas. Additionally, the client cannot continue with the remaining replicas because the client may read from an isolated replica. In our example, if a write operation is completed with only 2 replicas, the remaining replica can still serve the old values of X; X’’’ can be stale which is not strong consistency (real-time ordering is not there because one client can see the new value of X from X’ and another one can see the old value from X’’’ for example). This is the basis of a quorum which I will explain after the next one. Check <a href="https://www.cl.cam.ac.uk/teaching/1213/Databases/db2013_L11_L12_2up.pdf">this</a> and <a href="lsd.ls.fi.upm.es/Members/papers/2003/tods03.pdf">this</a>
			<br/>
			<br/>
<strong>2PC is “Anti-Availability”</strong> - One way to address the problem mentioned above is to remove the unresponsive replica from the replica group. However, this is not always straightforward, as network partitions can complicate matters and reliable failure detectors are needed. Even if you can remove the unresponsive replica, agreement among other replicas and clients on the new configuration is necessary, which may require stop-the-world reconfigurations that pause all clients from changing the replica configuration. This is why the 2PC protocol is referred to as the anti-availability protocol. For more information, check out Pat Helland’s note <a href="https://queue.acm.org/detail.cfm?id=2953944">(link to the note)<a/>. He has some great insights on the topic. There are various ways to address this problem, and quorums are one of the most popular methods. Papers such as Spanner have explored this approach. Other methods, such as primary-backup or chain replication, are also available, but I believe primary-backup replication is underrated in today's world.
			<br/>
			<br/>
<strong>Quorums</strong> - To avoid the 2PC anti-availability problem, one approach is to continue write operations with the remaining replicas, even if one fails. This approach forms the basis of quorums, which offer fault tolerance and eliminate the need for global reconfigurations. However, it's crucial that everyone agrees on what replicas are still available. While this approach is effective at tolerating network partitions, it comes with limitations. For instance, you cannot read from a single replica, and instead, you need to read from at least one replica that has the updated value. This is similar to the CP property in CAP theorem. It's worth noting that there are some issues with CAP, but these fall outside the scope of this discussion. Nevertheless, there are extended models that address these challenges.
			<br/>
			<br/>
<strong>Majority</strong> - If a write operation continues with only some of the replicas, the client's read must overlap with at least one of them. To ensure this, the majority of replicas are needed for write operations. This can be expressed as W+R>N. For example, if there are no failures, the write operation can reach all N replicas (W=N), and the client reads from its local replica (R=1), resulting in N+1 > N. If there is one possible failure, W must be N-1, and in that case, R must be at least 2. 
			<br/>
Since we usually don't know how many failures can occur, it's best to proceed with the majority. For example, if the write operation reaches at least N/2+1 replicas, the read should also read from N/2+1 (For any integer N>3, R=N/2+1 and W=N/2+1 implies R+W>N)
			<br/>
			<br/>
//{TODO- you can prove that systems with failures can never achieve R=1. Same equation can be used to prove that we need an atomic commit for when R=1.} 
			<br/>
			<br/>
<strong>Majority is not “Strong Consistency”</strong> - Simple majority based quorum protocol is not strongly consistent (For linearizable folks, it's not linearizable :). It violates real-time property without additional actions. 
			<br/>
			<br/>
Let's take our example with 3 replicas. If we use the majority every write (and read as well) operation needs to wait for 2 replicas to acknowledge before completing the operation. Let's assume client C1 is waiting for X in X’ and X’’ in our example, but not X’’’ due to network partitions. While updating those replicas, X’ and X’’ can well be 0 and 1. If another client C2 reads X=0 from X’ and X=1 from X’’ which it interprets as X=1(the newest value), it will see the new value of X. But if there is another client C3 that reads from X=0 from X’ and X=0 from X’’’ due to unresponsive X’’ (where X=1), then C3 only sees the old value X=0. You see, at a given time, two clients can see two different values which violate real-time property. This could be seen as an atomicity violation as the majority of replicas are not converged due to network delays.      
			<br/>
			<br/>
(NOTE- without majority quorums are not strongly consistent at all. For example check N>=5, W=(N-1), R=2). 
			<br/>
			<br/>
<strong>Read-Repair for Strong Consistency</strong>: However, we do not need atomic commitment to solve the above problem. For instance, in this example, if C2 sees the new value from X’’=1 it can wait and update X’ from its side which is called read-repair. This could simply provide real-time, hence atomicity, order to the write operations. C2 only continues and uses X=1 once it knows the majority is X=1. This way even if C reads X=0 from X’, that operation appears to have happened before C2’s read. Simple. This is the basic idea of ABD and MW-ABD protocol. It provides read/Write registers over quorum of replicas. (There are some problems with ABD/MW-ABD that I still do not fully understand.  In addition, some people call it linearizable. To further investigate, I checked in Lynch’s slidest at MIT. Prof.Lynch has never used, or called,  that ABD offers  lineraizability :). It says well-formedness and atomicity)
			<br/>
			<br/>
<strong>Quorums alleviate ‘agreement' problem (or atomic-consistency) for Reads/Writes </strong> - One observation in majority based quorums is that, because clients by-default read from more than one replica, it does not need agreement for simple reads/writes (recall in synchronous world both write and RMWs required agreement and we used atomic commitment or 2PC). For instance, Writes do not  need to block readers in order to atomically update all replicas (all copies of the same object). This is powerful; in other terms you do not need ‘agreement’ or ‘consensus’. This is an <font color="#ff0000">implementation artefact</font> of quorums because the read operation reads more than one replica. However, anything more than simple reads/writes, like read-modify-write (RMW), needs agreement between the order of operations (for instance the read and write in RMWs) in all copies.  It is proven in FLP impossibility.
			<br/>
			<br/>
Confusion? - Recall that atomic commitment is two-fold; committing multiple copies of the same object (type-1) or committing different objects(type-2). People have often used atomic-commitment for the latter.  But the former is also a form of atomic commitment which I have called ‘agreement or atomic-consistency’. However this method is only required for a single object/partition replicated database. 
			<br/>
			<br/>
<strong>Consensus = agreement (a form of atomic-consistency) over quorums</strong> - One last piece of the puzzle. We need ‘agreement’, or IMO, a form of atomic commitment, for any operation other than simple writes/reads to agree on the order of two (or multiple) operations in multiple copies of the object/data. Paxos proposes a complete consensus protocol that offers strong consistency; Paxos archives atomic-commitment over quorums. I think the way to realize this consistency model (as systems person) is as (strict) serializability because Paxos provides the real-time property and atomicity (multiple copies of the same object) for the operation. Not only that, it also provided recovery that does not (generally) stop the entire service. This is why Paxos is highly available and is the building block of almost all the distributed databases today. 
			<br/>
			<br/>
It might be confusing to use (strict) serializability because people often think strict serializability is only for transactions that have multiple objects/participants. That is why I believe people use ‘strong consistency’ (and linearizability in some cases) as the way to mimic, or perhaps intepret, its consistency; it actually implies real-time ordering and atomicity (multiple-copies of the same object). To me, Paxos looks like an (optimized) single-object transaction protocol. If you actually look at Paxos, it looks like an extension of the 2PC protocol I described before with an additional phase. (p.s. look at these papers that use strct serilizability to descrie paxos <a href="https://www.cs.cmu.edu/~dga/papers/leases-socc2014.pdf"> strong consistency</a>) 
			<br/>
			<br/>
(Similar to that 2PC protocol, Paxos, with some tweaks, should be able to support replicated single-partition mini-transactions with multiple objects. Note that RMW is two operations: but a read and a write to the same object. Anything more than a single partition needs multi-partition transactions. I am not gonna talk about it here. There is some work like 3PC, PAC etc. Also I like how systems like Spanner use consensus to make each partition highly available while running a multi-partition 2PC protocol for transactions. This is the other case apart from single-partition replication that 2PC is non-blocking. Check Spanner. it uses consensus within each replaictaed parition while using 2PC for transactions over different partitions. nice design!)  
			<br/>
			<br/>
<strong>Conclusion and Questions</strong>
			<br/>
There is a common misunderstanding, or confusion, about distributed systems consistency. Using linearizability as a way to describe available, or replicated, systems that serve client requests from different replicas, though compelling, it is not complete. The term strong consistency has come to the rescue, but many people, especially people in the industry, have (mis)interpreted it slightly differently.  Only a few people (like Brewer I guess :)) know about this. So next time you use strong consistency or linearizability, use it wisely. This is my opinion based on my research. Maybe there is something that I have missed or misunderstood. I welcome any comments, thoughts, or any constructive criticism. 
			<br/>			
			<br/>

Conclusion 1: ‘C’ in CAP is NOT linearizability. (also Brewer DID NOT called it linearizable :) ).
			<br/>
Conclusion 2: Strong consistency in consensus (Paxos) protocols is NOT linearizability. (though raft has used lin)
			<br/>
Conclusion 3: Strong consistency over a single object always DOES NOT mean linearizability. Its sometimes called, atomic consistency, one-copy serializability or strong consistency.
			<br/>
Conclusion 4: 2PC is NOT  only for transactions with multiple operations/participants. It can be ant aotmic commitment, e.g. single object replicated KVS. 
			<br/>
Conclusion 5: 2PC is NOT always blocking thus we need 3PC. 2PC is not blocking for single-parition single object replication, or with a reliable replication layer that is available (E.g Spanner, TiDB). 
			
			<br/>
			
			
p.s. Follwoing materials also substantiate my sentiment. [<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf"> linke1 </a>  <a href="https://cloud.google.com/spanner/docs/true-time-external-consistency">link2</a>]
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
			<br/>
		</p>
	</body>
</html>
